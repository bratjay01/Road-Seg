{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "zip_file_path = '/home/ubuntu/Cityscapes.zip'\n",
    "\n",
    "extraction_path = '/home/ubuntu/Cityscapes/'\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file to the specified directory\n",
    "    zip_ref.extractall(extraction_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_truth = \"/home/ubuntu/Cityscapes/gtFine\"\n",
    "dir_input = \"/home/ubuntu/Cityscapes/leftImg8bit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Target size of each sample in the dataset\n",
    "sample_size = (512, 256)\n",
    "\n",
    "# Directories for preprocessed datasets\n",
    "dir_truth_pp, dir_input_pp = (f'{d}_{sample_size[0]}_{sample_size[1]}' for d in (dir_truth, dir_input))\n",
    "\n",
    "# Run preprocessing\n",
    "for dir_full, dir_pp in ((dir_truth, dir_truth_pp), (dir_input, dir_input_pp)):\n",
    "    # Check if the directory already exists\n",
    "    if os.path.isdir(dir_pp):\n",
    "        print(f'Preprocessed directory already exists: {dir_pp}')\n",
    "        continue\n",
    "\n",
    "    print(f'Preprocessing: {dir_full}')\n",
    "\n",
    "    # Walk though the directory and preprocess each file \n",
    "    for root,_,files in  os.walk( dir_full ):\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f'Preprocessing sub-directory: {root.replace(dir_full, \"\")}')\n",
    "\n",
    "        # Create the directory in the preprocessed set\n",
    "        root_pp = root.replace(dir_full, dir_pp)\n",
    "        os.makedirs(root_pp, exist_ok=True)\n",
    "\n",
    "        for f in files:\n",
    "            if not f.endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            # Resize and save PNG image\n",
    "            path_original = os.path.join(root,f)\n",
    "            img_resized = Image.open(path_original).resize(sample_size, Image.NEAREST)\n",
    "            img_resized.save(path_original.replace(dir_full, dir_pp), 'png', quality=100)\n",
    "\n",
    "print(f'Preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes \n",
    "\n",
    "\n",
    "# train_dataset = Cityscapes('D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes', split='train', mode='fine',\n",
    "                    #  target_type='semantic')\n",
    "\n",
    "val_dataset  = Cityscapes('D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes',split='val', mode='fine', target_type='semantic')\n",
    "\n",
    "\n",
    "# test_dataset = Cityscapes('/home/ubuntu/Cityscapes/Cityscapes',split='test', mode='fine', target_type='semantic')\n",
    "\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colormap = mcolors.ListedColormap(['#000000', '#FF0000'])\n",
    "\n",
    "# Define a function to display an image and its mask\n",
    "def display_image_mask(image, mask,colormap):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Image')\n",
    "    \n",
    "    # Display the segmentation mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=colormap, vmin=0, vmax=1)  # Adjust vmin and vmax as needed\n",
    "    plt.title('Segmentation Mask')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def apply_colormap(mask, colormap):\n",
    "    # Create an RGB image from the mask using the colormap\n",
    "    height, width, num_classes = mask.shape\n",
    "    colorized_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_id, color in colormap.items():\n",
    "        class_pixels = (mask == class_id)\n",
    "        colorized_mask[class_pixels] = color\n",
    "    \n",
    "    return colorized_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesSearchDataset(torchvision.datasets.Cityscapes):\n",
    "    def __init__(self, *args,augmentation=None, preprocessing = None,**kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.semantic_target_type_index = [i for i, t in enumerate(self.target_type) if t == \"semantic\"][0]\n",
    "        self.colormap = self._generate_colormap()\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def _generate_colormap(self):\n",
    "        colormap = {}\n",
    "        for class_ in self.classes:\n",
    "            if class_.train_id in (-1, 255):\n",
    "                continue\n",
    "            colormap[class_.train_id] = class_.id\n",
    "        return colormap\n",
    "\n",
    "    # def _convert_to_segmentation_mask(self, mask):\n",
    "    #     height, width = mask.shape[:2]\n",
    "    #     segmentation_mask = np.zeros((height, width, len(self.colormap)), dtype=np.float32)\n",
    "    #     for label_index, label in self.colormap.items():\n",
    "    #         segmentation_mask[:, :, label_index] = (mask == label).astype(float)\n",
    "    #     return segmentation_mask\n",
    "\n",
    "    \n",
    "    def _convert_to_segmentation_mask(self, mask):\n",
    "        height, width = mask.shape[:2]\n",
    "        road_class_id = 7  # Replace with the actual class ID for 'road' in your dataset\n",
    "        road_mask = (mask == road_class_id).astype(np.float32)  # Assuming 7 is the class ID for 'road'\n",
    "        road_mask = np.expand_dims(road_mask, axis=-1)\n",
    "        return road_mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.targets[index][self.semantic_target_type_index], cv2.IMREAD_UNCHANGED)\n",
    "        # print(\"image\",image.shape)\n",
    "        mask = self._convert_to_segmentation_mask(mask)\n",
    "        \n",
    "        # print(\"mask\",mask.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CityscapesSearchDataset(\n",
    "    root='D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = custom_dataset[i]\n",
    "    display_image_mask(image, mask,custom_colormap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "\n",
    "\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        #albu.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.9),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.5),\n",
    "            \n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "        \n",
    "    ]\n",
    "    return albu.Compose(_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CityscapesSearchDataset(\n",
    "    root='D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    augmentation= get_training_augmentation()\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(5):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = custom_dataset[i]\n",
    "    display_image_mask(image, mask,custom_colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['road']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# @PipelineDecorator.component(return_values=['Training'])\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    activation=ACTIVATION,      # activation function, default is None\n",
    "    classes=len(CLASSES),\n",
    "                                   # define number of output labels\n",
    ")\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    # aux_params=aux_params\n",
    "    \n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = CityscapesSearchDataset(\n",
    "    root='D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    \n",
    ")\n",
    "\n",
    "valid_dataset = CityscapesSearchDataset(\n",
    "    root='D:/Academics/TUe_Modules/Avular/AED/Datasets/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import utils\n",
    "\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "# loss.__name__ = 'soft_bce'\n",
    "\n",
    "# loss = smp.losses.TverskyLoss(mode = 'binary', from_logits= True)   \n",
    "# loss.__name__ = 'Twersky_Loss'\n",
    "\n",
    "# loss = smp.losses.LovaszLoss(mode = 'binary', from_logits=True)   \n",
    "# loss.__name__ = 'Lovasz_Loss'\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "    # torch.optim.SGD( dict(params=model.parameters(), lr=0.1,momentum=0.9,weight_decay=0.0005))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "# logger = task.get_logger()\n",
    "for i in range(0, 50):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    print(train_logs)\n",
    "\n",
    "    # logger.report_scalar('Dice_loss train', 'model_output', iteration=i, value=train_logs['Dice_loss'])\n",
    "    # logger.report_scalar('IoU score train', 'model_output', iteration=i, value=train_logs['iou_score'])\n",
    "    # logger.report_scalar('Dice_loss valid', 'model_output', iteration=i, value=valid_logs['Dice_loss'])\n",
    "    # logger.report_scalar('IoU score valid', 'model_output', iteration=i, value=valid_logs['iou_score'])\n",
    "    \n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 40:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
