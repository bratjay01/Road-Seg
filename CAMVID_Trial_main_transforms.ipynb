{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **CamVid** dataset. It is a set of:\n",
    " - **train** images + segmentation masks\n",
    " - **validation** images + segmentation masks\n",
    " - **test** images + segmentation masks\n",
    " \n",
    "All images have 320 pixels height and 480 pixels width.\n",
    "For more inforamtion about dataset visit http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(project_name='Experimentation', task_name='torch_noaug_overfit_resnet101')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task.get_task(project_name='great project', task_name='experiment1')\n",
    "# task.mark_completed()\n",
    "#task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/CamVid/'\n",
    "\n",
    "# # load repo with data if it is not exists\n",
    "# if not os.path.exists(DATA_DIR):\n",
    "#     print('Loading data...')\n",
    "#     os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n",
    "#     print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'testannot')\n",
    "\n",
    "# x_test_dir = os.path.join(DATA_DIR, 'test_ood')\n",
    "# y_test_dir = os.path.join(DATA_DIR, 'testannot_ood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create new dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging: Did image load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Testing \n",
    "\n",
    " \n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "\n",
    "# # List all image files in the directory\n",
    "# image_files = [f for f in os.listdir(y_valid_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))]\n",
    "\n",
    "# # Loop through the image files and display them\n",
    "# for image_file in image_files:\n",
    "#     # Create the full file path\n",
    "#     image_path = os.path.join(y_valid_dir, image_file)\n",
    "    \n",
    "#     # Read and display the image using Matplotlib\n",
    "#     img = mpimg.imread(image_path)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(image_file)\n",
    "#     plt.axis('off')  # Turn off axis labels\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision import models, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import PipelineDecorator\n",
    "\n",
    "\n",
    "\n",
    "# @PipelineDecorator.component(return_values=['image'])\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "# @PipelineDecorator.component(cache=False,return_values=['image','mask'])    \n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "  \n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "       \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class CamVidDataset(torch.utils.data.Dataset):\n",
    "\n",
    "\n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "    \n",
    "    \n",
    "    def __init__(self, images_dir, masks_dir, augmentation=None,image_augmentation =None,mask_augmentation=None, image_preprocessing=None, mask_preprocessing=None,preprocessing = None, classes=None):\n",
    "        \n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in os.listdir(images_dir)]\n",
    "        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in os.listdir(masks_dir)]\n",
    "\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.image_augmentation = image_augmentation\n",
    "        self.mask_augmentation = mask_augmentation\n",
    "        self.image_preprocessing = image_preprocessing\n",
    "        self.mask_preprocessing = mask_preprocessing\n",
    "        self.preprocessing = preprocessing\n",
    "        self.classes = classes\n",
    "       \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(self.images_fps[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = Image.open(self.images_fps[idx]).convert(\"RGB\")\n",
    "        # image = np.array(image)\n",
    "        mask = Image.open(self.masks_fps[idx])\n",
    "\n",
    "        # Filter masks based on self.class_values\n",
    "        if self.class_values:\n",
    "            mask = np.array(mask)\n",
    "            filtered_mask = np.zeros_like(mask) \n",
    "            for cls_value in self.class_values:\n",
    "                class_mask = (mask == cls_value)\n",
    "                filtered_mask = np.logical_or(filtered_mask, class_mask)\n",
    "            mask = filtered_mask.astype('uint8')    #you change here\n",
    "\n",
    "      \n",
    "        # print(mask)\n",
    "        # print(mask.dtype)\n",
    "        # print(mask.shape)\n",
    "        # Create a PIL image from the numpy array\n",
    "        image_pil = Image.fromarray(image)\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "       \n",
    "   \n",
    "\n",
    "        if self.augmentation:\n",
    "                # Convert to PIL Image before augmentation\n",
    "            # image_pil = self.augmentation(image_pil)\n",
    "            # mask_pil = self.augmentation(mask_pil)\n",
    "            image_pil, mask_pil = self.augmentation(image_pil, mask_pil)\n",
    "\n",
    "                # Convert augmented PIL Image back to NumPy array\n",
    "            image = np.array(image_pil)\n",
    "            mask = np.array(mask_pil)\n",
    "            \n",
    "        # if self.image_augmentation:\n",
    "            \n",
    "        #     image_pil = Image.fromarray(image)\n",
    "        #     image_pil = self.image_augmentation(image_pil)\n",
    "        #     image = np.array(image_pil)\n",
    "        # # print(mask.dtype)\n",
    "        # # print(image.dtype)\n",
    "\n",
    "        # if self.mask_augmentation:\n",
    "        #     mask = mask*255\n",
    "        #     mask_pil = Image.fromarray(mask)\n",
    "        #     mask_pil = self.mask_augmentation(mask_pil)\n",
    "        #     mask = np.array(mask_pil)\n",
    "\n",
    "            \n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if self.preprocessing:\n",
    "        #     image = self.preprocessing(image)\n",
    "        #     # mask = mask[np.newaxis, ...].astype(np.float32)\n",
    "        #     mask = mask.transpose(2, 0, 1).astype('float32')\n",
    "       \n",
    "        # Apply preprocessing if provided\n",
    "        if self.image_preprocessing:\n",
    "            # image_transform, mask_transform = get_preprocessing(preprocessing_fn)\n",
    "            image = self.image_preprocessing(image)\n",
    "            \n",
    "        \n",
    "\n",
    "        if self.mask_preprocessing:\n",
    "            # print(mask.shape)\n",
    "            # image_transform, mask_transform = get_preprocessing(preprocessing_fn)\n",
    "            mask = self.mask_preprocessing(mask)\n",
    "             \n",
    "           \n",
    "\n",
    "        return image,mask\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "\n",
    "\n",
    "      # image = cv2.imread(self.images_fps[idx])\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # mask = cv2.imread(self.masks_fps[idx], 0)\n",
    "        \n",
    "        # # extract certain classes from mask (e.g. cars)\n",
    "        # masks = [(mask == v) for v in self.class_values]\n",
    "        # mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # # add background if mask is not binary\n",
    "        # if mask.shape[-1] != 1:\n",
    "        #     background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "        #     mask = np.concatenate((mask, background), axis=-1)\n",
    "\n",
    "        # Apply augmentation if providedb\n",
    "        # if self.augmentation:\n",
    "            # Convert to PIL Image before augmentation\n",
    "            # image, mask = self.augmentation(image, mask)\n",
    "\n",
    "            \n",
    "\n",
    "            # # Convert augmented PIL Image back to NumPy array\n",
    "            # image = np.array(image)\n",
    "            # mask = np.array(mask)\n",
    "\n",
    "\n",
    "\n",
    "  # if self.augmentation:\n",
    "        # # Convert to PIL Image before augmentation\n",
    "        #         image, mask = self.augmentation(image, mask)\n",
    "\n",
    "        #         # Convert augmented PIL Image back to NumPy array\n",
    "        #         image = np.array(image)\n",
    "        #         mask = np.array(mask)\n",
    "\n",
    "        #         # Apply preprocessing if provided\n",
    "        #         if self.preprocessing:\n",
    "        #             image = self.preprocessing(image)\n",
    "\n",
    "        #         image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "\n",
    "        #         # Convert the mask to the format (1, H, W)\n",
    "        #         mask = mask[np.newaxis, ...].astype(np.float32)\n",
    "        #         mask = torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "class CamVidDataset(torch.utils.data.Dataset):\n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "\n",
    "    def __init__(self, images_dir, masks_dir, augmentation=None, preprocessing=None, classes=None):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in os.listdir(images_dir)]\n",
    "        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in os.listdir(masks_dir)]\n",
    "\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.classes = classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = Image.open(self.images_fps[idx]).convert(\"RGB\")  # Ensure RGB channel order\n",
    "        mask = Image.open(self.masks_fps[idx])\n",
    "\n",
    "\n",
    "        # Filter masks based on self.class_values\n",
    "        if self.class_values:\n",
    "            filtered_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "            for cls_value in self.class_values:\n",
    "                class_mask = (mask == cls_value)\n",
    "                filtered_mask = np.logical_or(filtered_mask, class_mask)\n",
    "            mask = filtered_mask\n",
    "\n",
    "        if self.augmentation:\n",
    "            # Convert to PIL Image before augmentation\n",
    "            image, mask = self.augmentation(image, mask)\n",
    "\n",
    "            # Convert augmented PIL Image back to NumPy array\n",
    "            image = np.array(image)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Apply preprocessing if provided\n",
    "        if self.preprocessing:\n",
    "            image = self.preprocessing(image)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "\n",
    "\n",
    "\n",
    "# class CustomAugMixPreprocessing:\n",
    "#     def __init__(self, severity=3, mixture_width=3, alpha=1.0):\n",
    "#         self.augmix = torchvision.transforms.AugMix(severity=severity, mixture_width=mixture_width, alpha=alpha)\n",
    "\n",
    "#     def __call__(self, image, mask):\n",
    "#         # Apply AugMix to image\n",
    "#         augmix_image = self.augmix(image)\n",
    "\n",
    "#         return augmix_image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CamVidDataset(x_train_dir, y_train_dir, classes=['road'])\n",
    "\n",
    "image, mask = dataset[170] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    road_mask=mask[..., 0].squeeze(),\n",
    "    #pavement_mask=mask[..., 1].squeeze(),\n",
    "    #background_mask=mask[..., 2].squeeze(),\n",
    ")\n",
    "\n",
    "print(\"image shape is: \", image.shape,\"mask shape is : \", mask.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CamVidDataset(x_train_dir, y_train_dir, classes=['road'])\n",
    "\n",
    "# image, mask = dataset[170] # get some sample\n",
    "# visualize(\n",
    "#     image=image, \n",
    "#     road_mask=mask[..., 0].squeeze(),\n",
    "#     #pavement_mask=mask[..., 1].squeeze(),\n",
    "#     #background_mask=mask[..., 2].squeeze(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a powerful technique to increase the amount of your data and prevent model overfitting.  \n",
    "\n",
    "Albumentations pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from CAP_augmentation.src.cap_aug import CAP_Albu\n",
    "import albumentations as albu\n",
    "# from deepaugment.deepaugment import DeepAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @PipelineDecorator.component(return_values=['train_transform_aug'])\n",
    "\n",
    "import PIL\n",
    "\n",
    "def get_training_augmentation():\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.7),\n",
    "\n",
    "\n",
    "        # CAP_Albu(p=1, \n",
    "        #        source_images=x_train_dir, \n",
    "        #        n_objects_range=[10,20], \n",
    "        #        h_range=[100,101],\n",
    "        #        x_range=[500, 1500],\n",
    "        #        y_range=[600 ,1000],\n",
    "        #        class_idx=1),\n",
    "            \n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albu.Lambda(image=print_shape, mask= print_shape),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480),\n",
    "        albu.Lambda(image=print_shape, mask= print_shape)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=print_shape, mask= print_shape),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "        \n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to print the shape of the input\n",
    "def print_shape(x, **kwargs):\n",
    "    if isinstance(x, PIL.Image.Image):\n",
    "        print(\"Image size:\", x.size)\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        print(\"Array shape:\", x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randaugment trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuarAugment.muar.augmentations import AlbumentationsRandAugment\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transform(rand_augment, stage='train', size=(28,28)):\n",
    "        if stage == 'train':\n",
    "            resize_tfm = [albu.Resize(*size)]\n",
    "            \n",
    "            rand_tfms = rand_augment() # returns a list of transforms\n",
    "\n",
    "            tensor_tfms = [albu.Normalize(), ToTensorV2()]\n",
    "            return albu.Compose(resize_tfm + rand_tfms + tensor_tfms)\n",
    "\n",
    "        elif stage=='valid':\n",
    "            resize_tfm = [albu.Resize(*size)]\n",
    "            tensor_tfms = [albu.Normalize(), ToTensorV2()]\n",
    "            return albu.Compose(resize_tfm + tensor_tfms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandAugmentDataset(Dataset):\n",
    "    def __init__(self, data, stage='train', image_size=(28,28), N_TFMS=0, MAGN=0):\n",
    "        super().__init__()\n",
    "        self.images,self.labels = list(zip(*data))\n",
    "        self.stage, self.size = stage, image_size\n",
    "        self.N_TFMS, self.MAGN = N_TFMS, MAGN\n",
    "        if stage == 'train':\n",
    "            self.rand_augment = AlbumentationsRandAugment(N_TFMS, MAGN)\n",
    "        else: \n",
    "            self.rand_augment = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image,label = self.images[idx],self.labels[idx]\n",
    "        image = np.array(image)[:,:,None]\n",
    "        image = np.repeat(image, 3, axis=2) # image must be 3 channels\n",
    "        \n",
    "        transform = get_transform(self.rand_augment, self.stage, self.size)\n",
    "        augmented = transform(image=image)['image']\n",
    "        return augmented, torch.LongTensor([label])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_if_smaller(img, size, fill=0):\n",
    "    min_size = min(img.size)\n",
    "    if min_size < size:\n",
    "        ow, oh = img.size\n",
    "        padh = size - oh if oh < size else 0\n",
    "        padw = size - ow if ow < size else 0\n",
    "        img = F.pad(img, (0, 0, padw, padh), fill=fill)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomResize:\n",
    "    def __init__(self, min_size, max_size=None):\n",
    "        self.min_size = min_size\n",
    "        if max_size is None:\n",
    "            max_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        size = random.randint(self.min_size, self.max_size)\n",
    "        image = F.resize(image, size, antialias=True)\n",
    "        target = F.resize(target, size, interpolation=T.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = F.hflip(image)\n",
    "            target = F.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = pad_if_smaller(image, self.size)\n",
    "        target = pad_if_smaller(target, self.size, fill=255)\n",
    "        crop_params = T.RandomCrop.get_params(image, (self.size, self.size))\n",
    "        image = F.crop(image, *crop_params)\n",
    "        target = F.crop(target, *crop_params)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.center_crop(image, self.size)\n",
    "        target = F.center_crop(target, self.size)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class PILToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.pil_to_tensor(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToDtype:\n",
    "    def __init__(self, dtype, scale=False):\n",
    "        self.dtype = dtype\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if not self.scale:\n",
    "            return image.to(dtype=self.dtype), target\n",
    "        image = F.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class ColorJitter:\n",
    "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2):\n",
    "        self.transform = T.ColorJitter(\n",
    "            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GaussianBlur:\n",
    "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to a PyTorch tensor\n",
    "        image = F.to_tensor(image)\n",
    "\n",
    "        # Apply Gaussian blur\n",
    "        image = F.gaussian_blur(image, self.kernel_size, self.sigma)\n",
    "\n",
    "        # Convert the tensor back to a PIL image\n",
    "        image = F.to_pil_image(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomPerspective:\n",
    "    def __init__(self, distortion_scale=0.5, p=0.5):\n",
    "        self.distortion_scale = distortion_scale\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            # Generate random perspective transformation parameters\n",
    "            perspective_params = T.RandomPerspective.get_params(\n",
    "                width=image.width,\n",
    "                height=image.height,\n",
    "                distortion_scale=self.distortion_scale\n",
    "            )\n",
    "\n",
    "            # Apply perspective transformation to the image\n",
    "            transformed_image = F.perspective(image, *perspective_params)\n",
    "\n",
    "            # Apply the same perspective transformation to the mask\n",
    "            transformed_target = F.perspective(target, *perspective_params)\n",
    "\n",
    "            return transformed_image, transformed_target\n",
    "        else:\n",
    "            return image, target\n",
    "\n",
    "class CustomRandomApply:\n",
    "    def __init__(self, transforms, p):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            for transform in self.transforms:\n",
    "                image, target = transform(image, target)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    # Define your custom augmentations here\n",
    "   \n",
    "    random_resize = RandomResize(min_size=320, max_size=320)\n",
    "    random_horizontal_flip = RandomHorizontalFlip(flip_prob=0.5)\n",
    "    random_crop = RandomCrop(size=(320))\n",
    "    color_jitter = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    gaussian_blur = [GaussianBlur(kernel_size=3, sigma=0.5)]\n",
    "    gaussblur = CustomRandomApply(gaussian_blur, p=0.4)\n",
    "    custom_perspective = [RandomPerspective(distortion_scale=0.7)]\n",
    "    perspective = CustomRandomApply(custom_perspective, p=0.3)\n",
    "    # Create a list of your custom augmentations\n",
    "    custom_augmentations = [\n",
    "        random_resize,\n",
    "        random_horizontal_flip,\n",
    "        random_crop,\n",
    "        color_jitter,\n",
    "        gaussblur,\n",
    "        perspective,\n",
    "        # Add more custom augmentations as needed\n",
    "    ]\n",
    "\n",
    "    # Combine your custom augmentations into a single custom augmentation pipeline\n",
    "    custom_augmentation_pipeline = Compose(custom_augmentations)\n",
    "\n",
    "    return custom_augmentation_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision.transforms pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_training_augmentation():\n",
    "#     train_transform = transforms.Compose([\n",
    "#         # transforms.ToImage(),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "#         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "#         transforms.RandomApply([transforms.GaussianBlur(3, sigma=0.5)], p=0.2),\n",
    "#         transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.7)], p=0.2),\n",
    "#         # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         transforms.RandomResizedCrop((320, 320)),\n",
    "#         # transforms.ToDtype(torch.float32,scale=True),\n",
    "#     ])\n",
    "\n",
    "   \n",
    "#     return train_transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Pad((0,12)),\n",
    "        # transforms.RandomResizedCrop((384, 480)),\n",
    "        # transforms.ToDtype(torch.float32,scale=True)),\n",
    "    ])\n",
    "    return test_transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor(x,**kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "# # def custom_augmentation(image, mask):\n",
    "# #     augmented = get_training_augmentation()(image=image, mask=mask)\n",
    "# #     return augmented['image'], augmented['mask']\n",
    "\n",
    "# # # Create a custom transform using Lambda\n",
    "# # def custom_transform():\n",
    "    \n",
    "# #     return v2.Lambda(lambda x: custom_augmentation(x[0], x[1]))k\n",
    "\n",
    "# def get_preprocessing(preprocessing_fn):\n",
    "#     print(\"IN Preprocess\")\n",
    "    \n",
    "    \n",
    "#     transform = transforms.Compose([\n",
    "        \n",
    "#         transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "#         transforms.ToTensor(),\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "#     ])\n",
    "\n",
    "    \n",
    "\n",
    "#     return transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_preprocessing(preprocessing_fn):\n",
    "#     print(\"IN image  Preprocess\")\n",
    "\n",
    "#     _transform = transforms.Compose([\n",
    "#         transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "#         transforms.Lambda(lambda x: to_tensor(x)),\n",
    "#     ])\n",
    "\n",
    "    \n",
    "#     return _transform\n",
    "\n",
    "\n",
    "def get_preprocessing_image(preprocessing_fn):\n",
    "    print(\"IN image  Preprocess\")\n",
    "\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "        transforms.Lambda(lambda x: to_tensor(x)),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    return image_transform\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessing_mask(preprocessing_fn):\n",
    "    print(\"IN mask  Preprocess\")\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        # transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "        transforms.Lambda(lambda x: to_tensor(x)),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    return mask_transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize resulted augmented images and masks\n",
    "\n",
    "augmented_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    # preprocessing= get_preprocessing(preprocessing_fn),\n",
    "    classes=['road'],\n",
    ") \n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[35]\n",
    "    visualize(augmentedimage=image, mask=mask)\n",
    "    print(\"image shape is: \", image.shape,\"mask shape is : \", mask.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize resulted augmented images and masks\n",
    "\n",
    "# Now, you can use the custom transform in your dataset\n",
    "augmented_dataset = CamVidDataset(\n",
    "    x_train_dir,\n",
    "    y_train_dir,\n",
    "    # image_augmentation= get_image_training_augmentation(),\n",
    "    # mask_augmentation = get_mask_training_augmentation(),\n",
    "    augmentation= get_training_augmentation(),\n",
    "    classes=['road'],\n",
    ")\n",
    "\n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[50]\n",
    "    visualize(augmentedimage=image, mask=mask)\n",
    "    print(\"image shape is: \", image.shape,\"mask shape is : \", mask.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_dataset = CamVidDataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    #preprocessing= get_preprocessing(preprocessing_fn),\n",
    "    classes=['road'],\n",
    ") \n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[35]\n",
    "    print(\"image shape is: \", image.shape,\"mask shape is : \", mask.shape )\n",
    "    visualize(augmentedimage=image, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_dataset = Dataset(\n",
    "#     x_train_dir, \n",
    "#     y_train_dir, \n",
    "#     preprocessing=CustomAugMixPreprocessing(severity=3, mixture_width=3, alpha=1.0),\n",
    "#     augmentation=get_training_augmentation(),\n",
    "#     classes=['road'],\n",
    "# )\n",
    "# # same image with different random transforms\n",
    "# for i in range(3):\n",
    "#     image, mask = augmented_dataset[25]\n",
    "#     visualize(augmentedimage=image, mask=mask.squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'timm-regnetx_016'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['road']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# @PipelineDecorator.component(return_values=['Training'])\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.5,               # dropout ratio, default is None\n",
    "    activation=ACTIVATION,      # activation function, default is None\n",
    "    classes=len(CLASSES),\n",
    "                                   # define number of output labels\n",
    ")\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    # aux_params=aux_params\n",
    "    \n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = CamVidDataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    # image_preprocessing = get_preprocessing_image(preprocessing_fn),\n",
    "    # mask_preprocessing = get_preprocessing_mask(preprocessing_fn),\n",
    "    preprocessing = get_preprocessing(preprocessing_fn),\n",
    "    classes=['road'],\n",
    ") \n",
    "\n",
    "#same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[35]\n",
    "    print(\"image shape is: \", image.shape,\"mask shape is : \", mask.shape )\n",
    "    visualize(augmentedimage=image, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CamVidDataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    image_preprocessing = get_preprocessing_image(preprocessing_fn),\n",
    "    mask_preprocessing = get_preprocessing_mask(preprocessing_fn),\n",
    "    # preprocessing = get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = CamVidDataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    image_preprocessing = get_preprocessing_image(preprocessing_fn),\n",
    "    mask_preprocessing = get_preprocessing_mask(preprocessing_fn),\n",
    "    # preprocessing = get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD CUTMIX AND MIXUP AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix = transforms.CutMix(num_classes= CLASSES)\n",
    "mixup = transforms.MixUp(num_classes=CLASSES)\n",
    "cutmix_or_mixup = transforms.RandomChoice([cutmix, mixup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    # Check the shapes of images and masks\n",
    "    print(f\"Batch {batch_idx + 56}:\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Masks shape: {masks.shape}\")\n",
    "    \n",
    "    # Check the data types (should be torch tensors)\n",
    "    print(f\"Images data type: {type(images)}\")\n",
    "    print(f\"Masks data type: {type(masks)}\")\n",
    "    \n",
    "    # Optionally, you can visualize or print a sample image and mask\n",
    "    if batch_idx == 0:\n",
    "        sample_image = images[0]  # Take the first image in the batch\n",
    "        sample_mask = masks[0]    # Take the corresponding mask\n",
    "        # Visualize or print the sample image and mask to inspect their content\n",
    "        # ...\n",
    "\n",
    "    # Break the loop after inspecting a few batches\n",
    "    if batch_idx >= 2:  # Adjust the number of batches to inspect as needed\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (images, masks) in enumerate(valid_loader):\n",
    "    # Check the shapes of images and masks\n",
    "    print(f\"Batch {batch_idx + 56}:\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Masks shape: {masks.shape}\")\n",
    "    \n",
    "    # Check the data types (should be torch tensors)\n",
    "    print(f\"Images data type: {type(images)}\")\n",
    "    print(f\"Masks data type: {type(masks)}\")\n",
    "    \n",
    "    # Optionally, you can visualize or print a sample image and mask\n",
    "    if batch_idx == 0:\n",
    "        sample_image = images[0]  # Take the first image in the batch\n",
    "        sample_mask = masks[0]    # Take the corresponding mask\n",
    "        # Visualize or print the sample image and mask to inspect their content\n",
    "        # ...\n",
    "\n",
    "    # Break the loop after inspecting a few batches\n",
    "    if batch_idx >= 2:  # Adjust the number of batches to inspect as needed\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from segmentation_models_pytorch import utils\n",
    "\n",
    "# loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
    "# loss.__name__ = 'Dice_loss'\n",
    "loss = smp.losses.TverskyLoss(mode = 'binary')   \n",
    "loss.__name__ = 'Twersky_Loss'\n",
    "\n",
    "# loss = smp.losses.LovaszLoss(mode = 'binary')   \n",
    "# loss.__name__ = 'Lovasz_Loss'\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "    # torch.optim.SGD( dict(params=model.parameters(), lr=0.1,momentum=0.9,weight_decay=0.0005))\n",
    "])\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.SGD([ \n",
    "#     # {'params': model.parameters(), 'lr': 0.0001},  # Adam for other parameters\n",
    "#     {'params': model.parameters(), 'lr': 0.1, 'weight_decay': 0.0005, 'momentum': 0.9}  # SGD for model.parameters() with specific lr, weight_decay, and momentum\n",
    "# ])\n",
    "\n",
    "#task.connect(mutable = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.connect(mutable=loss,name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.connect(mutable=metrics,name='metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.connect(mutable = model,name='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. You can use GPU for computations.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU for computations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "# logger = task.get_logger()\n",
    "for i in range(0, 50):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    print(train_logs)\n",
    "\n",
    "    # logger.report_scalar('Dice_Loss train', 'model_output', iteration=i, value=train_logs['dice_loss'])\n",
    "    # logger.report_scalar('IoU score train', 'model_output', iteration=i, value=train_logs['iou_score'])\n",
    "    # logger.report_scalar('Dice_Loss valid', 'model_output', iteration=i, value=valid_logs['dice_loss'])\n",
    "    # logger.report_scalar('IoU score valid', 'model_output', iteration=i, value=valid_logs['iou_score'])\n",
    "    \n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './torch_lovasz_timm_016.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    # if i == 40:\n",
    "    #     optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    #     print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('torch_twersky_timm_016.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = CamVidDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    #preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    image_preprocessing = get_preprocessing_image(preprocessing_fn),\n",
    "    mask_preprocessing = get_preprocessing_mask(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)\n",
    "# logger.report_scalar('Test IoU', 'model_output', iteration=i, value=logs['iou_score'])\n",
    "# logger.report_scalar('Test Dice Loss', 'model_output', iteration=i, value=logs['dice_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = CamVidDataset(\n",
    "    x_test_dir, y_test_dir, \n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    # image_vis = np.array(test_dataset_vis[n][0]).astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate predictions and ground truth masks\n",
    "all_pr_masks = []\n",
    "all_gt_masks = []\n",
    "\n",
    "# Iterate through the entire test dataset\n",
    "for i in range(len(test_dataset)):\n",
    "    image, gt_mask = test_dataset[i]\n",
    "\n",
    "    gt_mask = gt_mask.squeeze()\n",
    "\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "    # Accumulate the predictions and ground truth masks\n",
    "    all_pr_masks.append(pr_mask)\n",
    "    all_gt_masks.append(gt_mask)\n",
    "\n",
    "# Convert the lists to tensors\n",
    "all_pr_masks = torch.from_numpy(np.array(all_pr_masks)).float()\n",
    "\n",
    "all_gt_masks = torch.from_numpy(np.array(all_gt_masks)).long()\n",
    "print(all_gt_masks.shape)\n",
    "print(all_pr_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "tp, fp, fn, tn = smp.metrics.get_stats(all_pr_masks, all_gt_masks, mode='binary', threshold=0.5)\n",
    "\n",
    "# Create and plot the confusion matrix\n",
    "conf_mat = ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "conf_matrix = conf_mat(all_pr_masks, all_gt_masks)\n",
    "conf_mat.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Recall, Accuracy,F1Score, FBetaScore,JaccardIndex\n",
    "\n",
    "\n",
    "# Calculate Intersection over Union (IoU)\n",
    "iou_score = JaccardIndex(task = \"binary\",num_classes=2)  # For a binary classification task\n",
    "iou_score.update(all_pr_masks,all_gt_masks)\n",
    "iou = iou_score.compute().item()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_metric = F1Score(task=\"binary\",num_classes=2)  # For a binary classification task\n",
    "f1_metric.update(all_pr_masks,all_gt_masks)\n",
    "f1_score = f1_metric.compute().item()\n",
    "\n",
    "# Calculate F2 score\n",
    "f2_metric = FBetaScore(task=\"binary\",beta=2.0, num_classes=2)  # For a binary classification task\n",
    "f2_metric.update(all_pr_masks,all_gt_masks)\n",
    "f2_score = f2_metric.compute().item()\n",
    "\n",
    "# Calculate Recall\n",
    "recall_metric = Recall(task = \"binary\",num_classes=2)  # For a binary classification task\n",
    "recall_metric.update(all_pr_masks,all_gt_masks)\n",
    "recall = recall_metric.compute().item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_metric = Accuracy(task = \"binary\",num_classes=2)  # For a binary classification task\n",
    "accuracy_metric.update(all_pr_masks,all_gt_masks)\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "\n",
    "# Print or use the calculated metrics as needed\n",
    "print(\"IoU:\", iou)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"F2 Score:\", f2_score)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score,precision_recall_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_true is the ground truth binary mask and y_prob is the probability map\n",
    "fpr, tpr, thresholds = roc_curve(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "auroc = roc_auc_score(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "\n",
    "\n",
    "desired_tpr = 0.935  # Set the desired TPR value\n",
    "index = next(i for i, value in enumerate(tpr) if value >= desired_tpr)\n",
    "\n",
    "fpr_at_desired_tpr = fpr[index]*100\n",
    "\n",
    "print(f\"FPR at {desired_tpr * 100}% TPR: {fpr_at_desired_tpr}\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUROC = {auroc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f'AUROC Score: {auroc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision,recall_val, thresholds = precision_recall_curve(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "aupr = auc(recall_val,precision)\n",
    "print(\"AUPR: \" , aupr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_val, precision, color='blue', lw=2, label='Precision-Recall Curve (AUPR = {:.2f})'.format(aupr))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
