{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.colors as mcolors\n",
    "from clearml import Task\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(project_name='Experimentation', task_name='exp5_ABX_noFT_aug_750')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_truth = \"/home/ubuntu/Datasets/abx_data_pfai/Mask_512_256\"\n",
    "dir_input = \"/home/ubuntu/Datasets/abx_data_pfai/Image_512_256\"\n",
    "dir_truth_val = \"/home/ubuntu/Datasets/abx_data_pfai/Val_mask_512_256\"\n",
    "dir_input_val = \"/home/ubuntu/Datasets/abx_data_pfai/Val_512_256\"\n",
    "dir_truth_test = \"/home/ubuntu/Datasets/abx_data_pfai/Test_mask_512_256\"\n",
    "dir_input_test = \"/home/ubuntu/Datasets/abx_data_pfai/Test_512_256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to copy specific number of images to the subset directory\n",
    "import shutil\n",
    "\n",
    "\n",
    "dir_truth = \"/home/ubuntu/Datasets/abx_data_pfai/Mask_512_256\"\n",
    "dir_input = \"/home/ubuntu/Datasets/abx_data_pfai/Image_512_256\"\n",
    "output_dir_truth_subset = \"/home/ubuntu/Mask_train_subset\"\n",
    "output_dir_input_subset = \"/home/ubuntu/Image_train_subset\"\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_dir_truth_subset, exist_ok=True)\n",
    "os.makedirs(output_dir_input_subset, exist_ok=True)\n",
    "\n",
    "mask_list = os.listdir(dir_truth)\n",
    "\n",
    "\n",
    "subset_indices = random.sample(range(len(mask_list)), 500)\n",
    "subset_masks = [mask_list[i] for i in subset_indices]\n",
    "\n",
    "# Copy the selected masks and corresponding images to the output directories\n",
    "for mask_name in subset_masks:\n",
    "    mask_path = os.path.join(dir_truth, mask_name)\n",
    "    image_path = os.path.join(dir_input, mask_name)\n",
    "\n",
    "    output_mask_path = os.path.join(output_dir_truth_subset, mask_name)\n",
    "    output_image_path = os.path.join(output_dir_input_subset, mask_name)\n",
    "\n",
    "    shutil.copy(mask_path, output_mask_path)\n",
    "    shutil.copy(image_path, output_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Target size of each sample in the dataset\n",
    "sample_size = (512, 256)\n",
    "\n",
    "# Directories for preprocessed datasets\n",
    "dir_truth_pp, dir_input_pp = (f'{d}_{sample_size[0]}_{sample_size[1]}' for d in (dir_truth, dir_input))\n",
    "\n",
    "# Run preprocessing\n",
    "for dir_full, dir_pp in ((dir_truth, dir_truth_pp), (dir_input, dir_input_pp)):\n",
    "    # Check if the directory already exists\n",
    "    if os.path.isdir(dir_pp):\n",
    "        print(f'Preprocessed directory already exists: {dir_pp}')\n",
    "        continue\n",
    "\n",
    "    print(f'Preprocessing: {dir_full}')\n",
    "\n",
    "    # Walk though the directory and preprocess each file \n",
    "    for root,_,files in  os.walk( dir_full ):\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f'Preprocessing sub-directory: {root.replace(dir_full, \"\")}')\n",
    "\n",
    "        # Create the directory in the preprocessed set\n",
    "        root_pp = root.replace(dir_full, dir_pp)\n",
    "        os.makedirs(root_pp, exist_ok=True)\n",
    "\n",
    "        for f in files:\n",
    "            if not f.endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            # Resize and save PNG image\n",
    "            path_original = os.path.join(root,f)\n",
    "            img_resized = Image.open(path_original).resize(sample_size, Image.NEAREST)\n",
    "            img_resized.save(path_original.replace(dir_full, dir_pp), 'png', quality=100)\n",
    "\n",
    "print(f'Preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes \n",
    "\n",
    "\n",
    "# train_dataset = Cityscapes('/home/ubuntu/Cityscapes/Cityscapes', split='train', mode='fine',\n",
    "                    #  target_type='semantic')\n",
    "\n",
    "# val_dataset  = Cityscapes('/home/ubuntu/Cityscapes/Cityscapes',split='val', mode='fine', target_type='semantic')\n",
    "\n",
    "\n",
    "# test_dataset = Cityscapes('/home/ubuntu/Cityscapes/Cityscapes',split='test', mode='fine', target_type='semantic')\n",
    "\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colormap = mcolors.ListedColormap(['#000000', '#FF0000'])\n",
    "\n",
    "# Define a function to display an image and its mask\n",
    "\n",
    "def overlay_transparent_mask(image, mask, transparency=0.8):\n",
    "    # Convert the image and mask to float for accurate calculations\n",
    "    image_float = image.astype(np.float32) / 255.0\n",
    "    mask_float = mask.astype(np.float32)\n",
    "\n",
    "    # Set the green channel to 1 where the road is present\n",
    "    image_float[:, :, 1] = np.maximum(image_float[:, :, 1], mask_float * transparency)\n",
    "\n",
    "    # Clip values to the valid range [0, 1]\n",
    "    image_float = np.clip(image_float, 0.0, 1.0)\n",
    "\n",
    "    # Convert back to uint8 for display\n",
    "    overlay_image = (image_float * 255).astype(np.uint8)\n",
    "\n",
    "    return overlay_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display_image_with_masks(image, gt_mask, predicted_mask, colormap):\n",
    "    # Convert the predicted mask to integer values (0 or 1)\n",
    "    predicted_mask = predicted_mask.astype(np.uint8)\n",
    "\n",
    "    overlay_image_gt = overlay_transparent_mask(image, gt_mask)\n",
    "    # Overlay transparent green mask on the original image\n",
    "    overlay_image = overlay_transparent_mask(image, predicted_mask)\n",
    "\n",
    "    plt.figure(figsize=(17, 10))\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Image')\n",
    "    \n",
    "\n",
    "    plt.imshow(overlay_image_gt)\n",
    "    plt.title('Ground truth Mask')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlay_image)\n",
    "    plt.title('Predicted Mask')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def apply_colormap(mask, colormap):\n",
    "    # Create an RGB image from the mask using the colormap\n",
    "    height, width, num_classes = mask.shape\n",
    "    colorized_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_id, color in colormap.items():\n",
    "        class_pixels = (mask == class_id)\n",
    "        colorized_mask[class_pixels] = color\n",
    "    \n",
    "    return colorized_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cityscapes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesSearchDataset(torchvision.datasets.Cityscapes):\n",
    "    def __init__(self, *args,augmentation=None, preprocessing = None,**kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.semantic_target_type_index = [i for i, t in enumerate(self.target_type) if t == \"semantic\"][0]\n",
    "        self.colormap = self._generate_colormap()\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def _generate_colormap(self):\n",
    "        colormap = {}\n",
    "        for class_ in self.classes:\n",
    "            if class_.train_id in (-1, 255):\n",
    "                continue\n",
    "            colormap[class_.train_id] = class_.id\n",
    "        return colormap\n",
    "\n",
    "    # def _convert_to_segmentation_mask(self, mask):\n",
    "    #     height, width = mask.shape[:2]\n",
    "    #     segmentation_mask = np.zeros((height, width, len(self.colormap)), dtype=np.float32)\n",
    "    #     for label_index, label in self.colormap.items():\n",
    "    #         segmentation_mask[:, :, label_index] = (mask == label).astype(float)\n",
    "    #     return segmentation_mask\n",
    "\n",
    "    \n",
    "    def _convert_to_segmentation_mask(self, mask):\n",
    "        height, width = mask.shape[:2]\n",
    "        road_class_id = 7  \n",
    "        road_mask = (mask == road_class_id).astype(np.float32) \n",
    "        road_mask = np.expand_dims(road_mask, axis=-1)\n",
    "        return road_mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.targets[index][self.semantic_target_type_index], cv2.IMREAD_UNCHANGED)\n",
    "        # print(\"image\",image.shape)\n",
    "        mask = self._convert_to_segmentation_mask(mask)\n",
    "        \n",
    "        # print(\"mask\",mask.shape)\n",
    "\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abx custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None, preprocessing=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_list = os.listdir(self.image_dir)\n",
    "        # print(f\"Total number of images: {len(self.image_list)}\")\n",
    "        \n",
    "        # Find indices of images with valid masks\n",
    "        self.valid_indices = []\n",
    "        for i, image_id in enumerate(self.image_list):\n",
    "            mask_path = os.path.join(self.mask_dir, image_id)\n",
    "            if os.path.exists(mask_path):\n",
    "                self.valid_indices.append(i)\n",
    "\n",
    "        # print(f\"Number of images with valid masks: {len(self.valid_indices)}\")\n",
    "\n",
    "        # Use valid indices to generate file paths\n",
    "        self.images_fps = [os.path.join(self.image_dir, self.image_list[i]) for i in self.valid_indices]\n",
    "        self.masks_fps = [os.path.join(self.mask_dir, self.image_list[i]) for i in self.valid_indices]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        index = self.valid_indices[i]\n",
    "        # print(f\"Index: {index}, Length of valid indices: {len(self.valid_indices)}\")\n",
    "        # print(f\"Length of images_fps: {len(self.images_fps)}, Length of masks_fps: {len(self.masks_fps)}\")\n",
    "        \n",
    "        image_path = self.images_fps[i]\n",
    "        mask_path = self.masks_fps[i]\n",
    "        # print(f\"Image path: {image_path}, Mask path: {mask_path}\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, 0).astype(np.uint8)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        mask = mask / 255.0\n",
    "        print(image)\n",
    "        # print(f\"Image shape: {image.shape}, Mask shape: {mask.shape}\")\n",
    "\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "       \n",
    "             \n",
    "           \n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dir_input, \n",
    "                        dir_truth,\n",
    "                        \n",
    "                        )\n",
    "\n",
    "\n",
    "for i in range(1):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = dataset[12]\n",
    "    visualize(image = image,\n",
    "              mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CityscapesSearchDataset(\n",
    "    root='/home/ubuntu/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(5):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = custom_dataset[i]\n",
    "    display_image_with_masks(image, mask,custom_colormap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation (albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "\n",
    "\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.3, always_apply=False, p=0.5),\n",
    "\n",
    "        albu.GaussNoise(p=0.4),\n",
    "        albu.Perspective(p=0.35),\n",
    "            \n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "        \n",
    "    ]\n",
    "    return albu.Compose(_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation (torchvision transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_if_smaller(img, size, fill=0):\n",
    "    min_size = min(img.size)\n",
    "    if min_size < size:\n",
    "        ow, oh = img.size\n",
    "        padh = size - oh if oh < size else 0\n",
    "        padw = size - ow if ow < size else 0\n",
    "        img = F.pad(img, (0, 0, padw, padh), fill=fill)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomResize:\n",
    "    def __init__(self, min_size, max_size=None):\n",
    "        self.min_size = min_size\n",
    "        if max_size is None:\n",
    "            max_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        size = random.randint(self.min_size, self.max_size)\n",
    "        image = F.resize(image, size, antialias=True)\n",
    "        target = F.resize(target, size, interpolation=T.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = F.hflip(image)\n",
    "            target = F.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = pad_if_smaller(image, self.size)\n",
    "        target = pad_if_smaller(target, self.size, fill=255)\n",
    "        crop_params = T.RandomCrop.get_params(image, (self.size, self.size))\n",
    "        image = F.crop(image, *crop_params)\n",
    "        target = F.crop(target, *crop_params)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class CenterCrop:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.center_crop(image, self.size)\n",
    "        target = F.center_crop(target, self.size)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class PILToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.pil_to_tensor(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToDtype:\n",
    "    def __init__(self, dtype, scale=False):\n",
    "        self.dtype = dtype\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if not self.scale:\n",
    "            return image.to(dtype=self.dtype), target\n",
    "        image = F.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class ColorJitter:\n",
    "    def __init__(self, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2):\n",
    "        self.transform = T.ColorJitter(\n",
    "            brightness=brightness, contrast=contrast, saturation=saturation, hue=hue\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GaussianBlur:\n",
    "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to a PyTorch tensor\n",
    "        image = F.to_tensor(image)\n",
    "\n",
    "        # Apply Gaussian blur\n",
    "        image = F.gaussian_blur(image, self.kernel_size, self.sigma)\n",
    "\n",
    "        # Convert the tensor back to a PIL image\n",
    "        image = F.to_pil_image(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomPerspective:\n",
    "    def __init__(self, distortion_scale=0.5, p=0.5):\n",
    "        self.distortion_scale = distortion_scale\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            # Generate random perspective transformation parameters\n",
    "            perspective_params = T.RandomPerspective.get_params(\n",
    "                width=image.width,\n",
    "                height=image.height,\n",
    "                distortion_scale=self.distortion_scale\n",
    "            )\n",
    "\n",
    "            # Apply perspective transformation to the image\n",
    "            transformed_image = F.perspective(image, *perspective_params)\n",
    "\n",
    "            # Apply the same perspective transformation to the mask\n",
    "            transformed_target = F.perspective(target, *perspective_params)\n",
    "\n",
    "            return transformed_image, transformed_target\n",
    "        else:\n",
    "            return image, target\n",
    "\n",
    "class CustomRandomApply:\n",
    "    def __init__(self, transforms, p):\n",
    "        self.transforms = transforms\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            for transform in self.transforms:\n",
    "                image, target = transform(image, target)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision augmentation pipeline (Used only for Experiment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_augmentation():\n",
    "  \n",
    "   \n",
    "    random_resize = RandomResize(min_size=320, max_size=320)\n",
    "    random_horizontal_flip = RandomHorizontalFlip(flip_prob=0.5)\n",
    "    random_crop = RandomCrop(size=(320))\n",
    "    color_jitter = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    gaussian_blur = [GaussianBlur(kernel_size=3, sigma=0.5)]\n",
    "    gaussblur = CustomRandomApply(gaussian_blur, p=0.4)\n",
    "    custom_perspective = [RandomPerspective(distortion_scale=0.7)]\n",
    "    perspective = CustomRandomApply(custom_perspective, p=0.5)\n",
    "    \n",
    "    custom_augmentations = [\n",
    "        random_resize,\n",
    "        random_horizontal_flip,\n",
    "        random_crop,\n",
    "        color_jitter,\n",
    "        gaussblur,\n",
    "        perspective,\n",
    "       \n",
    "    ]\n",
    "\n",
    "\n",
    "    custom_augmentation_pipeline = Compose(custom_augmentations)\n",
    "\n",
    "    return custom_augmentation_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480),\n",
    "        # albu.Lambda(image=print_shape, mask= print_shape)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessing_image(preprocessing_fn):\n",
    "    print(\"IN image  Preprocess\")\n",
    "\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "        transforms.Lambda(lambda x: to_tensor(x)),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    return image_transform\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessing_mask(preprocessing_fn):\n",
    "    print(\"IN mask  Preprocess\")\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        # transforms.Lambda(lambda x: preprocessing_fn(x)),\n",
    "        transforms.Lambda(lambda x: to_tensor(x)),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    return mask_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dir_input, \n",
    "                        dir_truth,\n",
    "                        augmentation=get_training_augmentation(),\n",
    "                        )\n",
    "\n",
    "\n",
    "for i in range(1):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = dataset[7]\n",
    "    visualize(image = image,\n",
    "              mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CityscapesSearchDataset(\n",
    "    root='/home/ubuntu/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    target_type='semantic',\n",
    "    augmentation= get_training_augmentation()\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(5):  # Change the range to the number of samples you want to visualize\n",
    "    image, mask = custom_dataset[i]\n",
    "    display_image_mask(image, mask,custom_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['road']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# # @PipelineDecorator.component(return_values=['Training'])\n",
    "# aux_params=dict(\n",
    "#     pooling='avg',             # one of 'avg', 'max'\n",
    "#     dropout=0.5,               # dropout ratio, default is None\n",
    "#     activation=ACTIVATION,      # activation function, default is None\n",
    "#     classes=len(CLASSES),\n",
    "#                                    # define number of output labels\n",
    "# )\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    # activation=ACTIVATION,\n",
    "    # aux_params=aux_params\n",
    "    \n",
    ")\n",
    "# Load the pre-trained weights into the model\n",
    "# model.load_state_dict(torch.load(\"/home/ubuntu/Cityscapes_noaug_weights.pth\"))\n",
    "\n",
    "# pretrained_model = torch.load(\"/home/ubuntu/Cityscapes_all_Aug.pth\")\n",
    "\n",
    "# Freeze the initial layers\n",
    "# freeze_layers = 20\n",
    "# for param in pretrained_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in pretrained_model.encoder[:freeze_layers].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# train_dataset = CityscapesSearchDataset(\n",
    "#     root='/home/ubuntu/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "#     split='train',\n",
    "#     mode='fine',\n",
    "#     target_type='semantic',\n",
    "#     # augmentation=get_training_augmentation(), \n",
    "#     preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    \n",
    "# )\n",
    "\n",
    "# valid_dataset = CityscapesSearchDataset(\n",
    "#     root='/home/ubuntu/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "#     split='val',\n",
    "#     mode='fine',\n",
    "#     target_type='semantic',\n",
    "#     preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(dir_input, \n",
    "                        dir_truth,\n",
    "                        augmentation = get_training_augmentation(),\n",
    "                        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                        )\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(dir_input_val,\n",
    "                              dir_truth_val,\n",
    "                              preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import utils\n",
    "\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "# loss.__name__ = 'soft_bce'\n",
    "\n",
    "# loss = smp.losses.TverskyLoss(mode = 'binary', from_logits= True)   \n",
    "# loss.__name__ = 'Twersky_Loss'\n",
    "\n",
    "# loss = smp.losses.LovaszLoss(mode = 'binary', from_logits=True)   \n",
    "# loss.__name__ = 'Lovasz_Loss'\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "    # torch.optim.SGD( dict(params=model.parameters(), lr=0.1,momentum=0.9,weight_decay=0.0005))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.connect(mutable = model,name='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), \"/home/ubuntu/all_Augment_finetunedabx.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "# logger = task.get_logger()\n",
    "for i in range(0, 30):\n",
    "    \n",
    "    # print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    print(train_logs)\n",
    "\n",
    "    # logger.report_scalar('Dice_loss train', 'model_output', iteration=i, value=train_logs['dice_loss'])\n",
    "    # logger.report_scalar('IoU score train', 'model_output', iteration=i, value=train_logs['iou_score'])\n",
    "    # logger.report_scalar('Dice_loss valid', 'model_output', iteration=i, value=valid_logs['dice_loss'])\n",
    "    # logger.report_scalar('IoU score valid', 'model_output', iteration=i, value=valid_logs['iou_score'])\n",
    "    \n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    \n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, '/home/ubuntu/NO_FT_ABXaug_FM_750_BCE.pth')\n",
    "        # torch.save(model.state_dict(), \"/home/ubuntu/NO_FT_ABXaug_weights_50.pth\")\n",
    "\n",
    "        print('Model saved!')\n",
    "        \n",
    "    # if i == 40:\n",
    "    #     optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    #     print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('/home/ubuntu/NO_FT_ABXaug_FM_750_BCE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(dir_input_test, \n",
    "                        dir_truth_test,\n",
    "                        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CityscapesSearchDataset(\n",
    "#     root='/home/ubuntu/Cityscapes/Cityscapes',  # Replace with the actual path to your dataset\n",
    "#     split='val',\n",
    "#     mode='fine',\n",
    "#     target_type='semantic',\n",
    "#     preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    \n",
    "# )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from skimage.metrics import structural_similarity as ssim    \n",
    "\n",
    "test_dataset_vis = CustomDataset(dir_input_test, \n",
    "                        dir_truth_test,\n",
    "                        \n",
    "                        )\n",
    "\n",
    "\n",
    "indices = [770,78,909,295]\n",
    "\n",
    "# for n in indices:\n",
    "for i in range(4):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "   \n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    # image_vis = np.array(test_dataset_vis[n][0]).astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    \n",
    "    ssim_score = ssim(gt_mask, pr_mask, data_range = 1,multichannel=True)\n",
    "    print(f\"SSIM Score for Image {i + 1}: {ssim_score}\")\n",
    "\n",
    "    image, mask = test_dataset_vis[i]\n",
    "    display_image_with_masks(image_vis, gt_mask, pr_mask, custom_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate predictions and ground truth masks\n",
    "all_pr_masks = []\n",
    "all_gt_masks = []\n",
    "SSIM_scores = []\n",
    "comparisons = []\n",
    "\n",
    "# Iterate through the entire test dataset\n",
    "for i in range(len(test_dataset)):\n",
    "    image, gt_mask = test_dataset[i]\n",
    "\n",
    "    gt_mask = gt_mask.squeeze()\n",
    "\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    ssim_score = ssim(gt_mask, pr_mask, data_range=1, multichannel=True)\n",
    "\n",
    "    \n",
    "    all_pr_masks.append(pr_mask)\n",
    "    all_gt_masks.append(gt_mask)\n",
    "    SSIM_scores.append(ssim_score)\n",
    "    comparisons.append((image_vis, gt_mask, pr_mask, ssim_score))\n",
    "\n",
    "all_pr_masks = torch.from_numpy(np.array(all_pr_masks)).float()\n",
    "\n",
    "all_gt_masks = torch.from_numpy(np.array(all_gt_masks)).long()\n",
    "print(all_gt_masks.shape)\n",
    "print(all_pr_masks.shape)\n",
    "print(np.mean(SSIM_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "tp, fp, fn, tn = smp.metrics.get_stats(all_pr_masks, all_gt_masks, mode='binary', threshold=0.5)\n",
    "\n",
    "\n",
    "conf_mat = ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "conf_matrix = conf_mat(all_pr_masks, all_gt_masks)\n",
    "conf_mat.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Recall, Accuracy,F1Score, FBetaScore,JaccardIndex\n",
    "\n",
    "\n",
    "# Calculate Intersection over Union (IoU)\n",
    "iou_score = JaccardIndex(task = \"binary\",num_classes=2)  \n",
    "iou_score.update(all_pr_masks,all_gt_masks)\n",
    "iou = iou_score.compute().item()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_metric = F1Score(task=\"binary\",num_classes=2)  \n",
    "f1_metric.update(all_pr_masks,all_gt_masks)\n",
    "f1_score = f1_metric.compute().item()\n",
    "\n",
    "# Calculate F2 score\n",
    "f2_metric = FBetaScore(task=\"binary\",beta=2.0, num_classes=2)  \n",
    "f2_metric.update(all_pr_masks,all_gt_masks)\n",
    "f2_score = f2_metric.compute().item()\n",
    "\n",
    "# Calculate Recall\n",
    "recall_metric = Recall(task = \"binary\",num_classes=2)  \n",
    "recall_metric.update(all_pr_masks,all_gt_masks)\n",
    "recall = recall_metric.compute().item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_metric = Accuracy(task = \"binary\",num_classes=2)  \n",
    "accuracy_metric.update(all_pr_masks,all_gt_masks)\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "\n",
    "print(\"IoU:\", iou)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"F2 Score:\", f2_score)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score,precision_recall_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "auroc = roc_auc_score(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "\n",
    "\n",
    "desired_tpr = 0.95  # Set the desired TPR value\n",
    "index = next(i for i, value in enumerate(tpr) if value >= desired_tpr)\n",
    "\n",
    "fpr_at_desired_tpr = fpr[index]*100\n",
    "\n",
    "print(f\"FPR at {desired_tpr * 100}% TPR: {fpr_at_desired_tpr}\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUROC = {auroc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f'AUROC Score: {auroc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision,recall_val, thresholds = precision_recall_curve(all_gt_masks.flatten(), all_pr_masks.flatten())\n",
    "aupr = auc(recall_val,precision)\n",
    "print(\"AUPR: \" , aupr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_val, precision, color='blue', lw=2, label='Precision-Recall Curve (AUPR = {:.2f})'.format(aupr))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = [0, 50, 100, 250, 500, 750]\n",
    "\n",
    "Test_IoU_FT_aug = [0, 91, 92.8, 95, 95.9, 95.7]\n",
    "Test_IoU_FT_noaug = [0, 93, 94, 95, 94.8, 95.6]\n",
    "Test_IoU_noFT = [0, 92.9, 94.4, 95, 95.5, 94.8]\n",
    "Test_IoU_noFTaug = [0, 93.5, 94.8, 94.65, 94.7, 94.3]\n",
    "\n",
    "# Plotting\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(train_size))\n",
    "\n",
    "plt.bar(index, Test_IoU_FT_aug, color='b', width=bar_width, label='Test_IoU_FT_aug')\n",
    "plt.bar(index + bar_width, Test_IoU_FT_noaug, color='g', width=bar_width, label='Test_IoU_FT_noaug')\n",
    "plt.bar(index + 2*bar_width, Test_IoU_noFT, color='r', width=bar_width, label='Test_IoU_noFT')\n",
    "plt.bar(index + 3*bar_width, Test_IoU_noFTaug, color='y', width=bar_width, label='Test_IoU_noFTaug')\n",
    "\n",
    "\n",
    "plt.xlabel('Train Size')\n",
    "plt.ylabel('Test IoU')\n",
    "plt.title('Test IoU for Different Scenarios')\n",
    "plt.xticks(index + 2*bar_width, train_size)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
